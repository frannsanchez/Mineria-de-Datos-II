{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frannsanchez/CloudService/blob/main/Cloud%20Service.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Y8z8XzhFVcMC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuracion"
      ],
      "metadata": {
        "id": "cWhiti4IQfk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "BASE_PATH      = \"content/\"\n",
        "\n",
        "LANDING_PATH   = BASE_PATH\n",
        "BRONZE_PATH    = f\"{BASE_PATH}/bronze\"\n",
        "SILVER_PATH    = f\"{BASE_PATH}/silver\"\n",
        "GOLD_PATH      = f\"{BASE_PATH}/gold\"\n",
        "CHECKPOINT_DIR = f\"{BASE_PATH}/checkpoints\"\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .appName(\"cloud-provider-analytics-mvp\")\n",
        "         .getOrCreate())\n"
      ],
      "metadata": {
        "id": "Ojj8RxwLpTJ6"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Bronze con 3 maestros\n",
        "\n"
      ],
      "metadata": {
        "id": "3eCvJfenpnYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema_customers = StructType([\n",
        "    StructField(\"org_id\", StringType(), True),\n",
        "    StructField(\"org_name\", StringType(), True),\n",
        "    StructField(\"region\", StringType(), True),\n",
        "    StructField(\"country\", StringType(), True),\n",
        "    StructField(\"industry\", StringType(), True),\n",
        "    StructField(\"org_created_at\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "schema_users = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"org_id\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"role\", StringType(), True),\n",
        "    StructField(\"created_at\", TimestampType(), True),\n",
        "    StructField(\"status\", StringType(), True)\n",
        "])\n",
        "\n",
        "schema_billing = StructType([\n",
        "    StructField(\"org_id\", StringType(), True),\n",
        "    StructField(\"billing_month\", StringType(), True),  # YYYY-MM\n",
        "    StructField(\"amount_usd\", DoubleType(), True),\n",
        "    StructField(\"credits_usd\", DoubleType(), True),\n",
        "    StructField(\"taxes_usd\", DoubleType(), True),\n",
        "    StructField(\"fx_rate\", DoubleType(), True)\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "3IdKsLEbpbU7"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSV a Bronze"
      ],
      "metadata": {
        "id": "sjinVIlWpr0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_to_bronze(csv_path: str,\n",
        "                  schema: StructType,\n",
        "                  pk_cols: list,\n",
        "                  bronze_subdir: str):\n",
        "\n",
        "    df = (spark.read\n",
        "          .option(\"header\", True)\n",
        "          .schema(schema)\n",
        "          .csv(csv_path))\n",
        "\n",
        "    df = (df\n",
        "          .withColumn(\"ingest_ts\", F.current_timestamp())\n",
        "          .withColumn(\"source_file\", F.input_file_name()))\n",
        "\n",
        "    total_raw = df.count()\n",
        "\n",
        "    if pk_cols:\n",
        "        df = df.dropDuplicates(pk_cols)\n",
        "\n",
        "    total_dedup = df.count()\n",
        "\n",
        "    print(f\"Archivo: {csv_path}\")\n",
        "    print(f\"Registros totales: {total_raw}\")\n",
        "    print(f\"Tras dedupe ({pk_cols}): {total_dedup}\")\n",
        "\n",
        "    df = df.withColumn(\"ingest_date\", F.to_date(\"ingest_ts\"))\n",
        "\n",
        "    out_path = f\"{BRONZE_PATH}/{bronze_subdir}\"\n",
        "    (df.write\n",
        "       .mode(\"overwrite\")\n",
        "       .partitionBy(\"ingest_date\")\n",
        "       .parquet(out_path))\n",
        "\n",
        "    print(f\"Escrito en Bronze: {out_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vAh7CnfhprLE"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutar cada maestro\n",
        "\n"
      ],
      "metadata": {
        "id": "K2x-DOR7qA3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# customers_orgs\n",
        "csv_to_bronze(\n",
        "    csv_path=f\"/content/customers_orgs.csv\",\n",
        "    schema=schema_customers,\n",
        "    pk_cols=[\"org_id\"],\n",
        "    bronze_subdir=\"customers_orgs\"\n",
        ")\n",
        "\n",
        "# users\n",
        "csv_to_bronze(\n",
        "    csv_path=f\"/content/users.csv\",\n",
        "    schema=schema_users,\n",
        "    pk_cols=[\"user_id\"],\n",
        "    bronze_subdir=\"users\"\n",
        ")\n",
        "\n",
        "# billing_monthly\n",
        "csv_to_bronze(\n",
        "    csv_path=f\"/content/billing_monthly.csv\",\n",
        "    schema=schema_billing,\n",
        "    pk_cols=[\"org_id\", \"billing_month\"],\n",
        "    bronze_subdir=\"billing_monthly\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tjz-WEkqFjF",
        "outputId": "3f815157-13ae-43a7-ea63-c03ba68bebbb"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo: /content/customers_orgs.csv\n",
            "Registros totales: 80\n",
            "Tras dedupe (['org_id']): 80\n",
            "Escrito en Bronze: content//bronze/customers_orgs\n",
            "Archivo: /content/users.csv\n",
            "Registros totales: 800\n",
            "Tras dedupe (['user_id']): 800\n",
            "Escrito en Bronze: content//bronze/users\n",
            "Archivo: /content/billing_monthly.csv\n",
            "Registros totales: 240\n",
            "Tras dedupe (['org_id', 'billing_month']): 240\n",
            "Escrito en Bronze: content//bronze/billing_monthly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Carpeta stream\n",
        "# Crear la carpeta de ingesta para eventos\n",
        "import os, shutil, glob\n",
        "\n",
        "BASE_PATH = \"/content\"\n",
        "events_dir = os.path.join(BASE_PATH, \"usage_events_stream\")\n",
        "os.makedirs(events_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "for f in glob.glob(os.path.join(BASE_PATH, \"events_part_*.jsonl\")):\n",
        "    shutil.move(f, events_dir)\n",
        "\n",
        "print(\"Archivos movidos a:\", events_dir)\n",
        "print(os.listdir(events_dir))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4K0zP2XfQ9P",
        "outputId": "1d8b3467-caa0-49d6-c7d4-38ba93f91524"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivos movidos a: /content/usage_events_stream\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streaming Bonze"
      ],
      "metadata": {
        "id": "uHblRDuJqRcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "BASE_PATH      = \"/content\"\n",
        "LANDING_PATH   = BASE_PATH\n",
        "BRONZE_PATH    = f\"{BASE_PATH}/bronze\"\n",
        "CHECKPOINT_DIR = f\"{BASE_PATH}/checkpoints\"\n",
        "\n",
        "schema_events = StructType([\n",
        "    StructField(\"event_id\", StringType(), True),\n",
        "    StructField(\"org_id\", StringType(), True),\n",
        "    StructField(\"service\", StringType(), True),\n",
        "    StructField(\"event_time\", TimestampType(), True),\n",
        "    StructField(\"unit\", StringType(), True),\n",
        "    StructField(\"value\", DoubleType(), True),\n",
        "    StructField(\"cost_usd_increment\", DoubleType(), True),\n",
        "    StructField(\"schema_version\", IntegerType(), True),\n",
        "    StructField(\"carbon_kg\", DoubleType(), True),\n",
        "    StructField(\"genai_tokens\", LongType(), True),\n",
        "])\n",
        "\n",
        "events_stream = (spark.readStream\n",
        "    .schema(schema_events)\n",
        "    .option(\"maxFilesPerTrigger\", 1)\n",
        "    .json(f\"{LANDING_PATH}/usage_events_stream\")   # ‚Üê ahora apunta a la carpeta nueva\n",
        "    .withWatermark(\"event_time\", \"2 hours\")\n",
        "    .dropDuplicates([\"event_id\"])\n",
        "    .withColumn(\"ingest_ts\", F.current_timestamp())\n",
        "    .withColumn(\"ingest_date\", F.to_date(\"ingest_ts\"))\n",
        ")\n",
        "\n",
        "bronze_events_path = f\"{BRONZE_PATH}/events\"\n",
        "events_checkpoint  = f\"{CHECKPOINT_DIR}/events_checkpoint\"\n",
        "\n",
        "query = (events_stream\n",
        "    .writeStream\n",
        "    .format(\"parquet\")\n",
        "    .option(\"checkpointLocation\", events_checkpoint)\n",
        "    .option(\"path\", bronze_events_path)\n",
        "    .partitionBy(\"service\", \"ingest_date\")\n",
        "    .outputMode(\"append\")\n",
        "    .start())\n"
      ],
      "metadata": {
        "id": "n2dSMYfsqUfV"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para detener el streaming:\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "KfX3AcH8uXXK"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events_bronze = spark.read.parquet(f\"{BRONZE_PATH}/events\")\n",
        "events_bronze.show(5)\n",
        "print(\"Filas en events_bronze:\", events_bronze.count())\n",
        "\n"
      ],
      "metadata": {
        "id": "lCOthsJYuiKW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "4fa89396-fb54-458c-ff0c-3462a247b398"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Unable to infer schema for Parquet at . It must be specified manually.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2307272154.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevents_bronze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{BRONZE_PATH}/events\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mevents_bronze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Filas en events_bronze:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents_bronze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    542\u001b[0m         )\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Unable to infer schema for Parquet at . It must be specified manually."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bronze Chek point"
      ],
      "metadata": {
        "id": "zDhZJZfMulrd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IZdajjXDulCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silver ‚Äì Enriquecimiento + Features + Calidad + Quarantine\n",
        "Leer Bronze\n"
      ],
      "metadata": {
        "id": "_O8Cxrd_u2h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "BASE_PATH    = \"/content\"\n",
        "BRONZE_PATH  = f\"{BASE_PATH}/bronze\"\n",
        "SILVER_PATH  = f\"{BASE_PATH}/silver\"\n",
        "\n"
      ],
      "metadata": {
        "id": "N2lrWZAfu5c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events_bronze = spark.read.parquet(f\"{BRONZE_PATH}/events\")\n",
        "customers_bronze = spark.read.parquet(f\"{BRONZE_PATH}/customers_orgs\")\n",
        "\n",
        "print(\"Events Bronze:\", events_bronze.count())\n",
        "print(\"Customers Bronze:\", customers_bronze.count())\n",
        "\n",
        "events_bronze.printSchema()\n",
        "customers_bronze.printSchema()\n"
      ],
      "metadata": {
        "id": "kXwl9WTUlym0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "customers_bronze_clean = customers_bronze.drop(\"ingest_ts\", \"source_file\", \"ingest_date\")\n",
        "customers_bronze_clean.printSchema()\n"
      ],
      "metadata": {
        "id": "JMVR9SScl2vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join para enriquecer"
      ],
      "metadata": {
        "id": "-R9OqiipvCXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join enriquecido\n",
        "events_enriched = (events_bronze.alias(\"e\")\n",
        "    .join(customers_bronze_clean.alias(\"c\"), on=\"org_id\", how=\"left\"))\n",
        "\n",
        "# Features\n",
        "events_feat = (events_enriched\n",
        "    .withColumn(\"daily_date\", F.to_date(\"event_time\"))\n",
        "    .withColumn(\"daily_cost_usd\", F.col(\"cost_usd_increment\"))\n",
        "    .withColumn(\n",
        "        \"requests\",\n",
        "        F.when(F.col(\"unit\") == F.lit(\"request\"), F.col(\"value\")).otherwise(F.lit(0.0))\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"genai_tokens_eff\",\n",
        "        F.coalesce(F.col(\"genai_tokens\"), F.lit(0))\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"cost_anomaly_flag\",\n",
        "        F.when(F.col(\"cost_usd_increment\") < -0.01, F.lit(1)).otherwise(F.lit(0))\n",
        "    )\n",
        ")\n",
        "\n",
        "events_feat.printSchema()\n",
        "events_feat.show(5)\n",
        "\n"
      ],
      "metadata": {
        "id": "gadMA-wTvCvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reglas del quarentine\n",
        "\n",
        "\n",
        "event_id no nulo\n",
        "\n",
        "cost_usd_increment >= -0.01\n",
        "\n",
        "unit no nulo si value no es nulo"
      ],
      "metadata": {
        "id": "THJXZg4ZvPJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_events = events_feat.count()\n",
        "print(\"Total events:\", total_events)\n",
        "\n",
        "valid = (events_feat\n",
        "    .filter(F.col(\"event_id\").isNotNull())\n",
        "    .filter(F.col(\"cost_anomaly_flag\") == 0)\n",
        "    .filter(~(F.col(\"value\").isNotNull() & F.col(\"unit\").isNull()))\n",
        ")\n",
        "\n",
        "valid_count = valid.count()\n",
        "print(\"Valid events:\", valid_count)\n",
        "\n",
        "quarantine = events_feat.subtract(valid)\n",
        "quarantine_count = quarantine.count()\n",
        "print(\"Quarantine events:\", quarantine_count)\n",
        "\n",
        "quarantine.show(10)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZUekTsslvPmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "problemas con ingest"
      ],
      "metadata": {
        "id": "f5NlRFSbvZv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_to_write = valid.drop(\"ingest_date\")\n",
        "quarantine_to_write = quarantine.drop(\"ingest_date\")\n",
        "\n",
        "print(\"Schema valid_to_write:\")\n",
        "valid_to_write.printSchema()\n"
      ],
      "metadata": {
        "id": "pUSbOCaPjNgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eventos v√°lidos\n",
        "(valid_to_write.write\n",
        " .mode(\"overwrite\")\n",
        " .partitionBy(\"daily_date\", \"service\")\n",
        " .parquet(f\"{SILVER_PATH}/events_valid\"))\n",
        "\n",
        "# Quarantine\n",
        "(quarantine_to_write.write\n",
        " .mode(\"overwrite\")\n",
        " .parquet(f\"{SILVER_PATH}/events_quarantine\"))\n"
      ],
      "metadata": {
        "id": "xhAI7Vghk8u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificamos\n",
        "events_valid = spark.read.parquet(f\"{SILVER_PATH}/events_valid\")\n",
        "events_quarantine = spark.read.parquet(f\"{SILVER_PATH}/events_quarantine\")\n",
        "\n",
        "print(\"events_valid:\", events_valid.count())\n",
        "print(\"events_quarantine:\", events_quarantine.count())\n",
        "events_valid.show(5)\n"
      ],
      "metadata": {
        "id": "mPxV33uCmeMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gold\n",
        "\n",
        "Primero leer el silver valido.\n"
      ],
      "metadata": {
        "id": "FXwicgV_vza9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/content\"\n",
        "SILVER_PATH = f\"{BASE_PATH}/silver\"\n",
        "GOLD_PATH = f\"{BASE_PATH}/gold\"\n",
        "\n",
        "events_valid = spark.read.parquet(f\"{SILVER_PATH}/events_valid\")\n",
        "events_valid.show(5)\n",
        "events_valid.printSchema()\n",
        "\n",
        "print(\"Eventos v√°lidos:\", events_valid.count())\n"
      ],
      "metadata": {
        "id": "NruQoiglvzvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregacion por org, por dia, por servicio (marts)"
      ],
      "metadata": {
        "id": "ROUd_1LYwA5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mart_finops = (events_valid\n",
        "    .groupBy(\"org_id\", \"daily_date\", \"service\")\n",
        "    .agg(\n",
        "        F.sum(\"daily_cost_usd\").alias(\"total_cost_usd\"),\n",
        "        F.sum(\"requests\").alias(\"total_requests\"),\n",
        "        F.sum(\"genai_tokens_eff\").alias(\"total_genai_tokens\"),\n",
        "        F.countDistinct(\"event_id\").alias(\"event_count\")\n",
        "    )\n",
        ")\n",
        "\n",
        "mart_finops.show(10)\n",
        "print(\"Total filas del mart:\", mart_finops.count())\n",
        "\n"
      ],
      "metadata": {
        "id": "VFpWkDOVwBKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escribir en el GOLD"
      ],
      "metadata": {
        "id": "3cOI4uHmwKag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#error para parar la ejeucion"
      ],
      "metadata": {
        "id": "snkDV6g6onsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DateType\n",
        "\n",
        "mart_finops_fixed = (mart_finops\n",
        "    .withColumn(\"daily_date\", F.col(\"daily_date\").cast(DateType()))\n",
        ")\n",
        "\n",
        "(mart_finops_fixed\n",
        " .write\n",
        " .mode(\"overwrite\")\n",
        " .partitionBy(\"daily_date\")\n",
        " .parquet(f\"{GOLD_PATH}/org_daily_usage_by_service\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "syNK_NTKwK0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparamos cassandra\n"
      ],
      "metadata": {
        "id": "NrlGdAncZeM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cassandra-driver"
      ],
      "metadata": {
        "id": "6yBv74Jtr-rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade astrapy"
      ],
      "metadata": {
        "id": "-XW6CTko1DMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conexion\n",
        "from astrapy import DataAPIClient\n",
        "\n",
        "# Initialize the client\n",
        "client = DataAPIClient(\"AstraCS:rZpUhtsQFZrdhwAJIueaxcsR:23c78df488ff1973d561967d2d504db9adbc7916cc7031b344aee54a189e498c\")\n",
        "db = client.get_database_by_api_endpoint(\n",
        "  \"https://3e6dbeb3-87d1-4e6e-8a08-42cf326af347-us-east1.apps.astra.datastax.com\"\n",
        ")\n",
        "\n",
        "print(f\"Connected to Astra DB: {db.list_collection_names()}\")"
      ],
      "metadata": {
        "id": "aJxIIc8LfAES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisar esta parte de cassandra que me costo mucho. Revisar tambien la parte GOLD"
      ],
      "metadata": {
        "id": "g5GIEd5NpEts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from astrapy import DataAPIClient\n",
        "\n",
        "ASTRA_TOKEN  = \"AstraCS:rZpUhtsQFZrdhwAJIueaxcsR:23c78df488ff1973d561967d2d504db9adbc7916cc7031b344aee54a189e498c\"\n",
        "API_ENDPOINT = \"https://3e6dbeb3-87d1-4e6e-8a08-42cf326af347-us-east1.apps.astra.datastax.com\"\n",
        "\n",
        "client = DataAPIClient(ASTRA_TOKEN)\n",
        "db = client.get_database(API_ENDPOINT)\n",
        "\n",
        "print(\"Colecciones actuales:\", db.list_collection_names())\n",
        "\n",
        "COLL_NAME = \"org_daily_usage_by_service_coll\"   # <<< NOMBRE NUEVO\n",
        "\n",
        "# Si la colecci√≥n no existe, la creo\n",
        "if COLL_NAME not in db.list_collection_names():\n",
        "    db.create_collection(COLL_NAME)\n",
        "\n",
        "# Ahora s√≠, obtengo la colecci√≥n\n",
        "collection = db.get_collection(COLL_NAME)\n",
        "print(\"Collection OK:\", collection.name)\n"
      ],
      "metadata": {
        "id": "MzQUgTtNmod-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Insertamos gold\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "GOLD_PATH = \"/content/gold\"\n",
        "\n",
        "mart_finops = spark.read.parquet(f\"{GOLD_PATH}/org_daily_usage_by_service\")\n",
        "mart_finops.printSchema()\n",
        "mart_finops.show(5)\n",
        "\n",
        "pdf = mart_finops.toPandas()\n",
        "\n",
        "# Para que DataAPI no se vuelva loco con el tipo fecha:\n",
        "if \"daily_date\" in pdf.columns:\n",
        "    pdf[\"daily_date\"] = pdf[\"daily_date\"].astype(str)\n",
        "\n",
        "rows = pdf.to_dict(orient=\"records\")\n",
        "len(rows)\n"
      ],
      "metadata": {
        "id": "tfhqGlqap2h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = collection.insert_many(rows)\n",
        "print(\"Documentos insertados:\", len(result.inserted_ids))\n"
      ],
      "metadata": {
        "id": "5dCqQD7tp_Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = collection.find(\n",
        "    projection={\"_id\": 0, \"org_id\": 1},\n",
        "    limit=20\n",
        ")\n",
        "\n",
        "org_ids = {d[\"org_id\"] for d in docs if \"org_id\" in d}\n",
        "print(\"ORG IDs disponibles:\", org_ids)\n"
      ],
      "metadata": {
        "id": "V-klmtJ3qLYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casandra\n",
        "Se ejecuta en el panel"
      ],
      "metadata": {
        "id": "XcxeYmL8wUpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#listamos para consulta\n",
        "\n",
        "docs_sample = collection.find(\n",
        "    filter={},\n",
        "    projection={\"_id\": 0, \"org_id\": 1},\n",
        "    limit=50\n",
        ")\n",
        "\n",
        "org_ids = sorted({d[\"org_id\"] for d in docs_sample if \"org_id\" in d})\n",
        "print(\"ORG IDs disponibles:\")\n",
        "for oid in org_ids:\n",
        "    print(\" -\", oid)\n"
      ],
      "metadata": {
        "id": "ADC7Rwq8s70s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tomo una muestra de documentos para ver qu√© org_id existen\n",
        "org_id = \"org_zbikcidk\"  # o, por ejemplo: org_id = \"org_123\"\n",
        "print(\"Trabajando con org_id =\", org_id)\n"
      ],
      "metadata": {
        "id": "K2i7BpqNtKIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "docs_evol = collection.find(\n",
        "    filter={\"org_id\": org_id},\n",
        "    projection={\n",
        "        \"_id\": 0,\n",
        "        \"daily_date\": 1,\n",
        "        \"service\": 1,\n",
        "        \"total_cost_usd\": 1,\n",
        "        \"total_requests\": 1,\n",
        "        \"total_genai_tokens\": 1\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Evoluci√≥n diaria del costo por servicio para org_id={org_id}:\")\n",
        "for d in docs_evol:\n",
        "    pprint(d)\n"
      ],
      "metadata": {
        "id": "G1W2NIJ-tnQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_all = collection.find(\n",
        "    filter={},\n",
        "    projection={\n",
        "        \"_id\": 0,\n",
        "        \"org_id\": 1,\n",
        "        \"total_cost_usd\": 1\n",
        "    }\n",
        ")\n",
        "\n",
        "df_all = pd.DataFrame(docs_all)\n",
        "\n",
        "top_orgs = (df_all\n",
        "    .groupby(\"org_id\", as_index=False)\n",
        "    .agg(total_cost_usd=(\"total_cost_usd\", \"sum\"))\n",
        "    .sort_values(\"total_cost_usd\", ascending=False)\n",
        "    .head(10)\n",
        ")\n",
        "\n",
        "print(\"Top 10 organizaciones por costo total (FinOps global):\")\n",
        "print(top_orgs)\n",
        "\n"
      ],
      "metadata": {
        "id": "SRylYQGitttm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OJo, No corran esto aca. No va dar bien, Es lenguaje CQL, el de casandra, como dijo Fran ayer\n",
        "\n",
        "#CREATE KEYSPACE IF NOT EXISTS cloud_analytics\n",
        "#WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
        "\n",
        "#CREATE TABLE IF NOT EXISTS cloud_analytics.org_daily_usage_by_service (\n",
        "  #org_id text,\n",
        "  #daily_date date,\n",
        "  #service text,\n",
        "  #total_cost_usd double,\n",
        "  #total_requests bigint,\n",
        "  #total_genai_tokens bigint,\n",
        "  #event_count bigint,\n",
        "  #PRIMARY KEY ((org_id, daily_date), service)\n",
        "#) WITH CLUSTERING ORDER BY (service ASC);\n"
      ],
      "metadata": {
        "id": "yWsE1le2wbP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "PglO8yqjr0A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "ASTRA_HOST = \"<TU_HOST_ASTRA>\"        # ej: \"xxx-xxx.apps.astra.datastax.com\"\n",
        "ASTRA_USER = \"<TOKEN_CLIENT_ID>\"\n",
        "ASTRA_PASS = \"<TOKEN_CLIENT_SECRET>\"\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "    .appName(\"cassandra-serving\")\n",
        "    .config(\"spark.cassandra.connection.host\", ASTRA_HOST)\n",
        "    .config(\"spark.cassandra.auth.username\", ASTRA_USER)\n",
        "    .config(\"spark.cassandra.auth.password\", ASTRA_PASS)\n",
        "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
        "    .getOrCreate())\n"
      ],
      "metadata": {
        "id": "5EmvsWtMw-vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el gold mart en casandra"
      ],
      "metadata": {
        "id": "9xjMtyguxOA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mart_finops = spark.read.parquet(f\"{GOLD_PATH}/org_daily_usage_by_service\")\n",
        "\n",
        "(mart_finops.write\n",
        " .format(\"org.apache.spark.sql.cassandra\")\n",
        " .mode(\"append\")\n",
        " .options(table=\"org_daily_usage_by_service\", keyspace=\"cloud_analytics\")\n",
        " .save())\n"
      ],
      "metadata": {
        "id": "H7Dzaum4w0hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estas serian consultas de negocios para evidencias\n",
        "\n"
      ],
      "metadata": {
        "id": "NwefnfBcxTGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "-- 1) Evoluci√≥n diaria del costo por servicio de una organizaci√≥n\n",
        "SELECT daily_date, service, total_cost_usd\n",
        "FROM cloud_analytics.org_daily_usage_by_service\n",
        "WHERE org_id = 'ORG_123'\n",
        "  AND daily_date >= '2025-01-01'\n",
        "  AND daily_date <= '2025-01-31';\n",
        "\n",
        "-- 2) Servicios con mayor costo en un rango de fechas\n",
        "SELECT service, total_cost_usd\n",
        "FROM cloud_analytics.org_daily_usage_by_service\n",
        "WHERE org_id = 'ORG_123'\n",
        "  AND daily_date >= '2025-01-01'\n",
        "  AND daily_date <= '2025-01-14';\n"
      ],
      "metadata": {
        "id": "s5TNfYtoxTcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba de Idempotencias"
      ],
      "metadata": {
        "id": "VwDVxVFm1xjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- INICIO PRUEBA DE IDEMPOTENCIA ---\")\n",
        "\n",
        "# 1. Conteo previo de la re-ejecuci√≥n\n",
        "# Leemos lo que ya existe en Bronze (Customers)\n",
        "df_before = spark.read.parquet(f\"{BRONZE_PATH}/customers_orgs\")\n",
        "count_before = df_before.count()\n",
        "print(f\"Registros en Bronze (Customers) - ANTES: {count_before}\")\n",
        "\n",
        "# 2. Re-ejecuci√≥n del proceso de ingesta\n",
        "# Volvemos a procesar el MISMO archivo de landing\n",
        "print(\"Re-procesando archivo 'customers_orgs.csv'...\")\n",
        "csv_to_bronze(\n",
        "    csv_path=f\"{LANDING_PATH}/customers_orgs.csv\",\n",
        "    schema=schema_customers,\n",
        "    pk_cols=[\"org_id\"],\n",
        "    bronze_subdir=\"customers_orgs\"\n",
        ")\n",
        "\n",
        "# 3. Conteo post re-ejecuci√≥n\n",
        "# Leemos nuevamente la ruta Bronze\n",
        "df_after = spark.read.parquet(f\"{BRONZE_PATH}/customers_orgs\")\n",
        "count_after = df_after.count()\n",
        "print(f\"Registros en Bronze (Customers) - DESPU√âS: {count_after}\")\n",
        "\n",
        "# 4. Validaci√≥n autom√°tica\n",
        "if count_before == count_after:\n",
        "    print(f\"√âXITO: Idempotencia verificada. {count_before} == {count_after}. No se generaron duplicados.\")\n",
        "else:\n",
        "    print(f\"FALLO: Se encontraron discrepancias. Antes: {count_before}, Despu√©s: {count_after}\")"
      ],
      "metadata": {
        "id": "Jyx7T6Jm14dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificaci√≥n de particionado sensato"
      ],
      "metadata": {
        "id": "3k2U0Wl56iF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def auditar_particiones(base_path, nombre_capa):\n",
        "    \"\"\"\n",
        "    Recorre la ruta dada, muestra las carpetas de partici√≥n creadas\n",
        "    y calcula el tama√±o total de los archivos Parquet.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\" AUDITOR√çA DE PARTICIONES: {nombre_capa}\")\n",
        "    print(f\" Ruta Base: {base_path}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    if not os.path.exists(base_path):\n",
        "        print(f\"La ruta {base_path} no existe. (¬øEjecutaste la escritura?)\")\n",
        "        return\n",
        "\n",
        "    # 1. Listar particiones (carpetas)\n",
        "    # Obtenemos items que sean directorios y no ocultos (como ._SUCCESS)\n",
        "    items = [i for i in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, i)) and not i.startswith(\".\")]\n",
        "    items.sort()\n",
        "\n",
        "    total_size = 0\n",
        "    file_count = 0\n",
        "\n",
        "    print(f\"üìÇ Estructura de Particiones encontradas ({len(items)}):\")\n",
        "\n",
        "    # Mostramos las primeras 5 particiones como evidencia visual\n",
        "    for partition in items[:5]:\n",
        "        print(f\"   ‚îú‚îÄ‚îÄ üìÅ {partition}\")\n",
        "\n",
        "    if len(items) > 5:\n",
        "        print(f\"   ‚îú‚îÄ‚îÄ ... (y {len(items)-5} particiones m√°s)\")\n",
        "\n",
        "    # 2. Calcular tama√±os (recorriendo recursivamente para sumar los .parquet)\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for f in files:\n",
        "            if f.endswith(\".parquet\"):\n",
        "                fp = os.path.join(root, f)\n",
        "                total_size += os.path.getsize(fp)\n",
        "                file_count += 1\n",
        "\n",
        "    # Convertir bytes a KB o MB para legibilidad\n",
        "    size_mb = total_size / (1024 * 1024)\n",
        "    print(f\"\\nüìä Resumen de Almacenamiento:\")\n",
        "    print(f\"   ‚Ä¢ Total de archivos Parquet: {file_count}\")\n",
        "    print(f\"   ‚Ä¢ Tama√±o total en disco: {size_mb:.4f} MB\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# --- EJECUCI√ìN DE LA AUDITOR√çA POR CAPA ---\n",
        "\n",
        "# 1. BRONZE (Batch) - Particionado por ingest_date\n",
        "auditar_particiones(f\"{BRONZE_PATH}/customers_orgs\", \"BRONZE: Customers (Batch)\")\n",
        "\n",
        "# 2. BRONZE (Streaming) - Particionado por service / ingest_date\n",
        "auditar_particiones(f\"{BRONZE_PATH}/events\", \"BRONZE: Events (Streaming)\")\n",
        "\n",
        "# 3. SILVER (Valid) - Particionado por daily_date / service (Clave para negocio)\n",
        "auditar_particiones(f\"{SILVER_PATH}/events_valid\", \"SILVER: Events Valid\")\n",
        "\n",
        "# 4. GOLD (Mart) - Particionado por daily_date (Optimizado para queries)\n",
        "auditar_particiones(f\"{GOLD_PATH}/org_daily_usage_by_service\", \"GOLD: FinOps Mart\")"
      ],
      "metadata": {
        "id": "aEPcs19b6psn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}